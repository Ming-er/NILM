{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -0- Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A Possible way to deprecate warnings. We might not use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#--------------------------------#\n",
    "# To use later\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "# To use later\n",
    "#--------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -1- Data Preparing & Cleaning\n",
    "#### - We encode the labels/targets to 0, n-1.\n",
    "#### - Read the Excel\n",
    "#### - If testing some selected phases only, we drop the right columns and define our features\n",
    "#### - Data Cleaning with Aggelos' Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "(26514, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kasloujr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\kasloujr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I50</th>\n",
       "      <th>Φ50</th>\n",
       "      <th>I150</th>\n",
       "      <th>Φ150</th>\n",
       "      <th>I250</th>\n",
       "      <th>Φ250</th>\n",
       "      <th>appliance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.619</td>\n",
       "      <td>2.6060</td>\n",
       "      <td>0.3099</td>\n",
       "      <td>-178.800</td>\n",
       "      <td>0.2033</td>\n",
       "      <td>5.325</td>\n",
       "      <td>PLAYR-TV+Tostiera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.572</td>\n",
       "      <td>0.6399</td>\n",
       "      <td>0.2669</td>\n",
       "      <td>-177.200</td>\n",
       "      <td>0.2273</td>\n",
       "      <td>2.255</td>\n",
       "      <td>PLAYR-TV+Tostiera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.573</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.2675</td>\n",
       "      <td>2.851</td>\n",
       "      <td>0.2269</td>\n",
       "      <td>-174.200</td>\n",
       "      <td>PLAYR-TV+Tostiera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.573</td>\n",
       "      <td>0.8874</td>\n",
       "      <td>0.2661</td>\n",
       "      <td>-178.200</td>\n",
       "      <td>0.2292</td>\n",
       "      <td>2.440</td>\n",
       "      <td>PLAYR-TV+Tostiera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.572</td>\n",
       "      <td>0.9411</td>\n",
       "      <td>0.2664</td>\n",
       "      <td>-178.100</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>3.350</td>\n",
       "      <td>PLAYR-TV+Tostiera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     I50     Φ50    I150     Φ150    I250     Φ250          appliance\n",
       "0  1.619  2.6060  0.3099 -178.800  0.2033    5.325  PLAYR-TV+Tostiera\n",
       "1  4.572  0.6399  0.2669 -177.200  0.2273    2.255  PLAYR-TV+Tostiera\n",
       "2  4.573  1.0000  0.2675    2.851  0.2269 -174.200  PLAYR-TV+Tostiera\n",
       "3  4.573  0.8874  0.2661 -178.200  0.2292    2.440  PLAYR-TV+Tostiera\n",
       "4  4.572  0.9411  0.2664 -178.100  0.2284    3.350  PLAYR-TV+Tostiera"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preparing\n",
    "# Encode labels with value 0-> n_classes -1\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Read excel with Pandas library\n",
    "path_to_dataset = \"../datasets/appliances_combination.xls\"\n",
    "#path_to_dataset= \"../datasets/one_appliance.xls\"\n",
    "\n",
    "# when you run it through azure notebooks\n",
    "#path_to_dataset= \"one_appliance.xls\"\n",
    "#path_to_dataset= \"appliances_combination.xls\"\n",
    "\n",
    "df = pd.read_excel(path_to_dataset)\n",
    "\n",
    "# Data Cleaning with Aggelos Rules for 50, 150, 250 phases\n",
    "df = df[(df.I50 > 0.1) & (df.I150 > 0.01) & (df.I250 > 0.01)] # Clean useless current features\n",
    "\n",
    "\n",
    "#df['Φ50'] = df['Φ50'].apply(pd.to_numeric)\n",
    "\n",
    "# For angle between (90, 180):\n",
    "# Modify by +180 degrees\n",
    "rows_with_rads_to_decrease = df.loc[(df['Φ50'] > 90) & (df['Φ50'] < 180)]\n",
    "rows_with_rads_to_decrease['Φ50'] -= 180\n",
    "df.update(rows_with_rads_to_decrease)\n",
    "\n",
    "# For angle between (-180, -90):\n",
    "# Modify by -180 degrees\n",
    "rows_with_rads_to_increase = (df.loc[(df['Φ50'] < -90) & (df['Φ50'] > -180)])\n",
    "rows_with_rads_to_increase['Φ50'] += 180\n",
    "df.update(rows_with_rads_to_increase)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(df.shape)\n",
    "\n",
    "#print(df.shape)\n",
    "\n",
    "# These columns will be our features - X\n",
    "#features = ['I50', 'Φ50',]# 'I150', 'Φ150', 'I250', 'Φ250'] # # 50 Version\n",
    "#features = ['I150', 'Φ150',] # 150\n",
    "#features = ['I50', 'Φ50', 'I150', 'Φ150'] # 50-150\n",
    "#features = ['I250', 'Φ250'] #250\n",
    "#features = ['I50', 'Φ50', 'I150', 'Φ150',]# 'I250', 'Φ250'] # 50-150 Version\n",
    "features = ['I50', 'Φ50', 'I150', 'Φ150', 'I250', 'Φ250'] # 50-150-250 Version\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are going to test some selected phases, then you are going to remove some columns\n",
    "\n",
    "#df = df.drop(df.columns[cols],axis=1,inplace=True)\n",
    "#removeColumns = ['I150', 'Φ150', 'I250', 'Φ250'] # 50 \n",
    "#removeColumns = ['I50', 'Φ50', 'I250', 'Φ250'] # 150\n",
    "#removeColumns = ['I50', 'Φ50', 'I150', 'Φ150'] # 250\n",
    "#removeColumns = [ 'I250', 'Φ250'] # 50-150\n",
    "#columns = [ 'I250', 'Φ250'] \n",
    "#df.drop(removeColumns, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -2- Data Scaling-Normalizing \n",
    "#### There is standard scaling, min-max scaling, robust scaling (preferred one) and normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73248368 0.32116261 0.00890449 0.91041454 0.28352734 0.14491286]\n",
      " [0.41940973 0.2027718  0.04913007 0.90068722 0.10816284 0.09461722]\n",
      " [0.41979981 0.22445561 0.04856879 0.19394722 0.11108559 2.79623488]\n",
      " ...\n",
      " [0.10305838 0.63946472 0.15040756 1.20771107 1.40874006 0.54965345]\n",
      " [0.14635678 0.93982244 0.18324286 0.87819277 1.35759208 0.40777717]\n",
      " [0.23061312 0.82179896 0.14030439 0.85205058 1.41531622 0.42776436]]\n",
      "(array([    2,    73,    94, ..., 26482, 26497, 26506], dtype=int64), array([5, 5, 5, ..., 5, 1, 1], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Features - X\n",
    "X = df[features]\n",
    "\n",
    "# Calculate Z-score in order to find outliers\n",
    "z = np.abs(stats.zscore(X))\n",
    "print(z) # Visualize\n",
    "threshold = 2.5 # Change the threshold arbitrarily\n",
    "print(np.where(z > threshold))\n",
    "df = df[(z < threshold).all(axis=1)] # Remove outliers that exceed the threshold given from dataset\n",
    "\n",
    "# Now get as X the 'clean' features\n",
    "X = df[features]\n",
    "\n",
    "# Scale the inputs (4 options; put comment in the ones you are not using)\n",
    "\n",
    "# -1- Standard Scaling\n",
    "#scaler = StandardScaler()\n",
    "#X = scaler.fit_transform(X)\n",
    "\n",
    "# -2- Min-max scaling \n",
    "#scaler = preprocessing.MinMaxScaler()\n",
    "#X = scaler.fit_transform(X)\n",
    "\n",
    "# -3- Robust scaling\n",
    "scaler = preprocessing.RobustScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# -4- Normalizing\n",
    "#scaler = preprocessing.Normalizer()\n",
    "#X = scaler.fit_transform(X)\n",
    "\n",
    "# Label - Y\n",
    "y = df['appliance']\n",
    "\n",
    "# Convert y to integer \n",
    "le.fit(y) # Fit label encoder\n",
    "y = le.transform(y) # Transform labels to normalized encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -3- Classification (Method 1: Cross-validation)\n",
    "## WARNING: DOES NOT WORK YET IF YOU WANT ALL THE METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that runs cross validation by giving as parameter the classification method\n",
    "# X,y, the score type (i.e 'accuracy', 'f1_score') and number of folds\n",
    "# Returns the score.\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "def cross_val(method, X, y, kfold_num):\n",
    "    return cross_val_score(method, X, y, scoring='f1_score', cv=kfold_num)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state = 42)\n",
    "#clf = LogisticRegression()\n",
    "print(cross_val(clf, X, y, 10))\n",
    "\n",
    "clf = LogisticRegression()\n",
    "print(cross_val(clf, X, y, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -3- Classification (Method 2: Train/Test Splitting)\n",
    "\n",
    "#### printMetrics method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMetrics(clf, X_test, y_test):\n",
    "    # Uncomment if you want confusion matrix to be shown\n",
    "    #cm = confusion_matrix(y_test, y_pred) \n",
    "    #print(\"Confusion matrix\")\n",
    "    #print(cm)\n",
    "    \n",
    "    \n",
    "    # This will print precision, recall, f1-score, support for all the categories\n",
    "    #target_names = ['class 0', 'class 1', 'class 2']\n",
    "    print(\"Classification report for classifier \\n%s:\\n%s\" % (clf, metrics.classification_report(y_test, y_pred)))\n",
    "    print(\"Accuracy: %1.3f\" % clf.score(X_test, y_test))\n",
    "    print(\"-----------------\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -3.1- Classification Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "\n",
      "\n",
      "Classification report for classifier \n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.39      0.95      0.55       118\n",
      "          1       0.41      0.22      0.29       144\n",
      "          2       0.69      0.71      0.70       132\n",
      "          3       0.00      0.00      0.00       130\n",
      "          4       0.25      0.34      0.29        53\n",
      "          5       0.30      0.16      0.21        93\n",
      "          6       0.23      0.06      0.10        96\n",
      "          7       0.49      0.29      0.37       141\n",
      "          8       0.00      0.00      0.00        51\n",
      "          9       0.40      0.90      0.56        86\n",
      "         10       0.00      0.00      0.00        39\n",
      "         11       0.00      0.00      0.00        46\n",
      "         12       0.00      0.00      0.00        40\n",
      "         13       0.00      0.00      0.00        33\n",
      "         14       0.00      0.00      0.00        52\n",
      "         15       0.00      0.00      0.00        28\n",
      "         16       0.26      0.42      0.32        36\n",
      "         17       0.00      0.00      0.00        14\n",
      "         18       0.00      0.00      0.00        24\n",
      "         19       0.73      0.87      0.80       334\n",
      "         20       0.41      0.40      0.41       228\n",
      "         21       0.53      0.76      0.63       284\n",
      "         22       0.58      0.70      0.63       301\n",
      "         23       0.00      0.00      0.00       126\n",
      "         24       0.57      0.16      0.26        97\n",
      "         25       0.00      0.00      0.00       193\n",
      "         26       0.34      0.66      0.45       280\n",
      "         27       0.31      0.47      0.37       186\n",
      "         28       0.00      0.00      0.00       197\n",
      "         29       0.58      0.69      0.63       116\n",
      "         30       0.00      0.00      0.00       156\n",
      "         31       0.49      0.83      0.62       175\n",
      "         32       0.00      0.00      0.00       157\n",
      "         33       0.22      0.94      0.36       157\n",
      "         34       0.54      0.85      0.66        47\n",
      "         35       0.60      0.53      0.56        78\n",
      "         36       0.31      0.19      0.24        57\n",
      "\n",
      "avg / total       0.34      0.44      0.36      4525\n",
      "\n",
      "Accuracy: 0.436\n",
      "-----------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kasloujr\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Split to train & test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform Logistic Regression\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression\\n\\n\")\n",
    "printMetrics(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Trees:\n",
      "Classification report for classifier \n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
      "            splitter='best'):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.97      0.98       118\n",
      "          1       0.85      0.81      0.83       144\n",
      "          2       0.94      0.92      0.93       132\n",
      "          3       0.82      0.83      0.82       130\n",
      "          4       0.91      0.94      0.93        53\n",
      "          5       0.97      0.92      0.95        93\n",
      "          6       0.93      0.94      0.93        96\n",
      "          7       0.98      0.96      0.97       141\n",
      "          8       0.76      0.76      0.76        51\n",
      "          9       0.92      0.91      0.91        86\n",
      "         10       1.00      1.00      1.00        39\n",
      "         11       0.95      0.91      0.93        46\n",
      "         12       0.93      1.00      0.96        40\n",
      "         13       0.89      0.97      0.93        33\n",
      "         14       0.98      0.94      0.96        52\n",
      "         15       0.96      0.96      0.96        28\n",
      "         16       0.97      0.94      0.96        36\n",
      "         17       0.82      1.00      0.90        14\n",
      "         18       0.86      0.79      0.83        24\n",
      "         19       0.90      0.90      0.90       334\n",
      "         20       0.87      0.86      0.86       228\n",
      "         21       0.93      0.96      0.95       284\n",
      "         22       0.90      0.88      0.89       301\n",
      "         23       0.86      0.85      0.85       126\n",
      "         24       0.97      0.94      0.95        97\n",
      "         25       0.88      0.84      0.86       193\n",
      "         26       0.88      0.91      0.89       280\n",
      "         27       0.89      0.92      0.90       186\n",
      "         28       0.89      0.86      0.87       197\n",
      "         29       0.92      0.90      0.91       116\n",
      "         30       0.91      0.92      0.91       156\n",
      "         31       0.93      0.95      0.94       175\n",
      "         32       0.81      0.85      0.83       157\n",
      "         33       0.91      0.89      0.90       157\n",
      "         34       0.85      0.94      0.89        47\n",
      "         35       0.93      0.90      0.92        78\n",
      "         36       0.86      0.88      0.87        57\n",
      "\n",
      "avg / total       0.90      0.90      0.90      4525\n",
      "\n",
      "Accuracy: 0.901\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Trees\n",
    "clf = DecisionTreeClassifier(random_state = 42) # Feel free to change 'min_samples_split' \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Decision Trees:\")\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "printMetrics(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-layer Perceptron:\n",
      "Classification report for classifier \n",
      "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(18,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=42, shuffle=True,\n",
      "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.97      0.93       118\n",
      "          1       0.61      0.58      0.59       144\n",
      "          2       0.79      0.70      0.75       132\n",
      "          3       0.46      0.49      0.48       130\n",
      "          4       0.49      0.66      0.56        53\n",
      "          5       0.71      0.65      0.68        93\n",
      "          6       0.72      0.94      0.81        96\n",
      "          7       0.93      0.91      0.92       141\n",
      "          8       0.53      0.69      0.60        51\n",
      "          9       0.70      0.37      0.48        86\n",
      "         10       0.93      1.00      0.96        39\n",
      "         11       0.94      1.00      0.97        46\n",
      "         12       0.87      0.82      0.85        40\n",
      "         13       0.70      0.85      0.77        33\n",
      "         14       0.74      0.81      0.77        52\n",
      "         15       0.64      0.96      0.77        28\n",
      "         16       0.76      0.53      0.62        36\n",
      "         17       0.91      0.71      0.80        14\n",
      "         18       0.73      0.46      0.56        24\n",
      "         19       0.75      0.87      0.80       334\n",
      "         20       0.85      0.79      0.82       228\n",
      "         21       0.79      0.86      0.83       284\n",
      "         22       0.84      0.75      0.79       301\n",
      "         23       0.75      0.44      0.56       126\n",
      "         24       0.86      0.85      0.85        97\n",
      "         25       0.71      0.79      0.75       193\n",
      "         26       0.60      0.85      0.70       280\n",
      "         27       0.65      0.46      0.54       186\n",
      "         28       0.67      0.46      0.54       197\n",
      "         29       0.64      0.89      0.74       116\n",
      "         30       0.80      0.54      0.65       156\n",
      "         31       0.84      0.91      0.87       175\n",
      "         32       0.71      0.37      0.49       157\n",
      "         33       0.69      0.93      0.79       157\n",
      "         34       0.85      0.85      0.85        47\n",
      "         35       0.64      0.72      0.68        78\n",
      "         36       0.52      0.39      0.44        57\n",
      "\n",
      "avg / total       0.74      0.73      0.72      4525\n",
      "\n",
      "Accuracy: 0.730\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multi-layer Perceptron\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(18,), random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Multi-layer Perceptron:\")\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "printMetrics(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-Nearest Neighbors\n",
      "Classification report for classifier \n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform'):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.95      0.91       118\n",
      "          1       0.74      0.76      0.75       144\n",
      "          2       0.85      0.91      0.88       132\n",
      "          3       0.81      0.85      0.83       130\n",
      "          4       0.76      0.91      0.83        53\n",
      "          5       0.89      0.87      0.88        93\n",
      "          6       0.81      0.88      0.84        96\n",
      "          7       0.96      0.97      0.96       141\n",
      "          8       0.64      0.75      0.69        51\n",
      "          9       0.75      0.74      0.75        86\n",
      "         10       0.95      1.00      0.97        39\n",
      "         11       0.92      0.96      0.94        46\n",
      "         12       0.98      1.00      0.99        40\n",
      "         13       0.86      0.97      0.91        33\n",
      "         14       0.80      0.75      0.77        52\n",
      "         15       0.74      0.89      0.81        28\n",
      "         16       0.79      0.75      0.77        36\n",
      "         17       0.70      1.00      0.82        14\n",
      "         18       0.77      0.42      0.54        24\n",
      "         19       0.86      0.88      0.87       334\n",
      "         20       0.81      0.78      0.79       228\n",
      "         21       0.92      0.93      0.93       284\n",
      "         22       0.95      0.83      0.89       301\n",
      "         23       0.85      0.84      0.85       126\n",
      "         24       0.92      0.91      0.91        97\n",
      "         25       0.86      0.84      0.85       193\n",
      "         26       0.82      0.89      0.85       280\n",
      "         27       0.83      0.80      0.82       186\n",
      "         28       0.86      0.78      0.82       197\n",
      "         29       0.73      0.70      0.71       116\n",
      "         30       0.92      0.89      0.91       156\n",
      "         31       0.94      0.93      0.93       175\n",
      "         32       0.71      0.66      0.68       157\n",
      "         33       0.80      0.88      0.84       157\n",
      "         34       0.90      0.94      0.92        47\n",
      "         35       0.86      0.81      0.83        78\n",
      "         36       0.75      0.75      0.75        57\n",
      "\n",
      "avg / total       0.85      0.85      0.85      4525\n",
      "\n",
      "Accuracy: 0.849\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k-Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"k-Nearest Neighbors\")\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "printMetrics(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machines (C-Support Vector)\n",
      "Classification report for classifier \n",
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=2, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.97      0.94       118\n",
      "          1       0.84      0.76      0.80       144\n",
      "          2       0.81      0.91      0.85       132\n",
      "          3       0.78      0.78      0.78       130\n",
      "          4       0.74      0.92      0.82        53\n",
      "          5       0.95      0.84      0.89        93\n",
      "          6       0.76      0.94      0.84        96\n",
      "          7       0.98      0.96      0.97       141\n",
      "          8       0.90      0.69      0.78        51\n",
      "          9       0.88      0.58      0.70        86\n",
      "         10       0.95      1.00      0.97        39\n",
      "         11       0.94      0.96      0.95        46\n",
      "         12       1.00      1.00      1.00        40\n",
      "         13       0.89      0.94      0.91        33\n",
      "         14       0.76      0.71      0.73        52\n",
      "         15       0.68      1.00      0.81        28\n",
      "         16       0.74      0.64      0.69        36\n",
      "         17       0.74      1.00      0.85        14\n",
      "         18       1.00      0.21      0.34        24\n",
      "         19       0.82      0.90      0.86       334\n",
      "         20       0.60      0.86      0.70       228\n",
      "         21       0.95      0.92      0.93       284\n",
      "         22       0.96      0.80      0.88       301\n",
      "         23       0.93      0.75      0.83       126\n",
      "         24       0.86      0.91      0.88        97\n",
      "         25       0.77      0.83      0.80       193\n",
      "         26       0.65      0.91      0.76       280\n",
      "         27       0.95      0.53      0.68       186\n",
      "         28       0.81      0.65      0.72       197\n",
      "         29       0.65      0.90      0.75       116\n",
      "         30       0.97      0.65      0.78       156\n",
      "         31       0.95      0.93      0.94       175\n",
      "         32       0.80      0.38      0.52       157\n",
      "         33       0.68      0.92      0.78       157\n",
      "         34       0.91      0.91      0.91        47\n",
      "         35       0.81      0.77      0.79        78\n",
      "         36       0.76      0.65      0.70        57\n",
      "\n",
      "avg / total       0.83      0.81      0.81      4525\n",
      "\n",
      "Accuracy: 0.813\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machines (C-Support Vector)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(gamma=2, C=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Support Vector Machines (C-Support Vector)\")\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "printMetrics(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kasloujr\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier \n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.92      0.71       118\n",
      "          1       0.53      0.43      0.47       144\n",
      "          2       0.79      0.73      0.76       132\n",
      "          3       0.78      0.29      0.42       130\n",
      "          4       0.26      0.60      0.37        53\n",
      "          5       0.67      0.91      0.78        93\n",
      "          6       0.60      0.64      0.62        96\n",
      "          7       0.81      0.95      0.87       141\n",
      "          8       1.00      0.08      0.15        51\n",
      "          9       0.54      0.76      0.63        86\n",
      "         10       1.00      0.74      0.85        39\n",
      "         11       0.00      0.00      0.00        46\n",
      "         12       0.97      0.88      0.92        40\n",
      "         13       0.94      0.45      0.61        33\n",
      "         14       0.00      0.00      0.00        52\n",
      "         15       0.52      0.79      0.63        28\n",
      "         16       0.00      0.00      0.00        36\n",
      "         17       0.00      0.00      0.00        14\n",
      "         18       0.00      0.00      0.00        24\n",
      "         19       0.77      0.96      0.85       334\n",
      "         20       0.64      0.71      0.67       228\n",
      "         21       0.59      0.87      0.70       284\n",
      "         22       0.93      0.81      0.87       301\n",
      "         23       0.85      0.22      0.35       126\n",
      "         24       0.85      0.85      0.85        97\n",
      "         25       0.63      0.77      0.70       193\n",
      "         26       0.51      0.98      0.67       280\n",
      "         27       0.54      0.38      0.45       186\n",
      "         28       1.00      0.01      0.01       197\n",
      "         29       0.66      0.69      0.67       116\n",
      "         30       0.83      0.24      0.38       156\n",
      "         31       0.67      0.95      0.78       175\n",
      "         32       0.70      0.24      0.36       157\n",
      "         33       0.56      0.85      0.67       157\n",
      "         34       0.78      0.89      0.83        47\n",
      "         35       0.64      0.83      0.72        78\n",
      "         36       0.58      0.46      0.51        57\n",
      "\n",
      "avg / total       0.68      0.65      0.61      4525\n",
      "\n",
      "Accuracy: 0.653\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=5, n_estimators=100, max_features=\"auto\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Random Forests\")\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "printMetrics(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuadraticDiscriminantAnalysis Classifier\n",
      "Classification report for classifier \n",
      "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,\n",
      "               store_covariance=False, store_covariances=None, tol=0.0001):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.95      0.93       118\n",
      "          1       0.48      0.21      0.29       144\n",
      "          2       0.71      0.70      0.71       132\n",
      "          3       0.60      0.43      0.50       130\n",
      "          4       0.61      0.96      0.75        53\n",
      "          5       0.77      0.66      0.71        93\n",
      "          6       0.76      0.93      0.84        96\n",
      "          7       0.90      0.87      0.88       141\n",
      "          8       0.39      0.55      0.46        51\n",
      "          9       0.75      0.64      0.69        86\n",
      "         10       1.00      1.00      1.00        39\n",
      "         11       0.93      0.89      0.91        46\n",
      "         12       0.91      1.00      0.95        40\n",
      "         13       1.00      0.97      0.98        33\n",
      "         14       0.96      0.87      0.91        52\n",
      "         15       0.88      1.00      0.93        28\n",
      "         16       1.00      0.97      0.99        36\n",
      "         17       1.00      0.93      0.96        14\n",
      "         18       0.94      0.67      0.78        24\n",
      "         19       0.50      0.95      0.65       334\n",
      "         20       0.47      0.34      0.40       228\n",
      "         21       0.67      0.85      0.75       284\n",
      "         22       0.68      0.17      0.27       301\n",
      "         23       0.53      0.37      0.44       126\n",
      "         24       0.64      0.93      0.76        97\n",
      "         25       0.17      0.04      0.07       193\n",
      "         26       0.51      0.97      0.67       280\n",
      "         27       0.61      0.34      0.44       186\n",
      "         28       0.88      0.04      0.07       197\n",
      "         29       0.67      0.73      0.70       116\n",
      "         30       0.83      0.28      0.41       156\n",
      "         31       0.86      0.85      0.85       175\n",
      "         32       0.60      0.08      0.14       157\n",
      "         33       0.28      0.97      0.44       157\n",
      "         34       0.64      0.83      0.72        47\n",
      "         35       0.63      0.46      0.53        78\n",
      "         36       0.53      0.63      0.58        57\n",
      "\n",
      "avg / total       0.64      0.60      0.56      4525\n",
      "\n",
      "Accuracy: 0.599\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# QuadraticDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "clf = QuadraticDiscriminantAnalysis()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"QuadraticDiscriminantAnalysis Classifier\")\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "printMetrics(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposal: An iterating way of evaluating classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(clf)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    printMetrics(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
