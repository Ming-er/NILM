{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45594385 0.16532287 0.0946546  0.94436727 0.02791146 0.14068024]\n",
      " [0.33151664 0.11597225 0.09209629 0.93473174 0.01952717 0.09073563]\n",
      " [0.33178331 4.39312762 0.09213199 0.14957256 0.0196669  2.77994068]\n",
      " ...\n",
      " [0.11525167 0.29800554 0.10478672 1.15376947 0.05299699 0.54259607]\n",
      " [0.14485144 0.42320819 0.10687502 0.91244958 0.05055157 0.40170997]\n",
      " [0.20245099 0.37401068 0.10414417 0.88655409 0.0533114  0.42155767]]\n",
      "(array([    2,     2,    73, ..., 27660, 27660, 27677]), array([1, 5, 1, ..., 1, 5, 1]))\n",
      "Logistic Regression:\n",
      "\n",
      "Accuracy: 0.431\n",
      "Precision: 0.386\n",
      "Recall: 0.431\n",
      "F1-Score: 0.353\n",
      "-----------------\n",
      "\n",
      "Naive Bayes Bernoulli:\n",
      "\n",
      "Accuracy: 0.358\n",
      "Precision: 0.282\n",
      "Recall: 0.358\n",
      "F1-Score: 0.289\n",
      "-----------------\n",
      "\n",
      "Decision Trees:\n",
      "\n",
      "Accuracy: 0.888\n",
      "Precision: 0.889\n",
      "Recall: 0.888\n",
      "F1-Score: 0.888\n",
      "-----------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klajdi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/klajdi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "#--------------------------------#\n",
    "# To use later\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "# To use later\n",
    "#--------------------------------#\n",
    "    \n",
    "# Encode labels with value 0-> n_classes -1\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Read excel with Pandas library\n",
    "path_to_dataset = \"../datasets/appliances_combination.xls\"\n",
    "#path_to_dataset= \"../datasets/one_appliance.xls\"\n",
    "\n",
    "df = pd.read_excel(path_to_dataset)\n",
    "\n",
    "# These columns will be our features - X\n",
    "features = ['I50', 'Φ50', 'I150', 'Φ150', 'I250', 'Φ250']\n",
    "\n",
    "# Features - X\n",
    "X = df[features]\n",
    "\n",
    "# Calculate Z-score in order to find outliers\n",
    "z = np.abs(stats.zscore(X))\n",
    "print(z) # Visualize\n",
    "threshold = 2.5 # Change the threshold arbitrarily\n",
    "print(np.where(z > threshold))\n",
    "df = df[(z < threshold).all(axis=1)] # Remove outliers that exceed the threshold given from dataset\n",
    "\n",
    "# Now get as X the 'clean' features\n",
    "X = df[features]\n",
    "\n",
    "# Scale the inputs (4 options; put comment in the ones you are not using)\n",
    "\n",
    "# -1- Standard Scaling\n",
    "#scaler = StandardScaler()\n",
    "#X = scaler.fit_transform(X)\n",
    "\n",
    "# -2- Min-max scaling \n",
    "#scaler = preprocessing.MinMaxScaler()\n",
    "#X = scaler.fit_transform(X)\n",
    "\n",
    "# -3- Robust scaling\n",
    "scaler = preprocessing.RobustScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# -4- Normalizing\n",
    "#scaler = preprocessing.Normalizer()\n",
    "#X = scaler.fit_transform(X)\n",
    "\n",
    "# Label - Y\n",
    "y = df['appliance']\n",
    "\n",
    "# Convert y to integer \n",
    "le.fit(y) # Fit label encoder\n",
    "y = le.transform(y) # Transform labels to normalized encoding\n",
    "\n",
    "# Split to train & test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform Logistic Regression\n",
    "reg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"\\nAccuracy: %1.3f\" % reg.score(X_test, y_test))\n",
    "print(\"Precision: %1.3f\" % metrics.precision_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"Recall: %1.3f\" % metrics.recall_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"F1-Score: %1.3f\" % metrics.f1_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"-----------------\\n\")\n",
    "\n",
    "# Bayesian Ridge\n",
    "#bayess = BayesianRidge().fit(X_train, y_train)\n",
    "\n",
    "#y_pred = bayess.predict(X_test)\n",
    "#print(\"Bayesian Ridge:\\n\")\n",
    "#print(\"Accuracy: %1.3f\" % bayess.score(X_test, y_test))\n",
    "#print(\"Precision: %1.3f\" % metrics.precision_score(y_test, y_pred, average ='weighted'))\n",
    "#print(\"Recall: %1.3f\" % metrics.recall_score(y_test, y_pred, average ='weighted'))\n",
    "#print(\"F1-Score: %1.3f\" % metrics.f1_score(y_test, y_pred, average ='weighted'))\n",
    "#print(\"-----------------\\n\")\n",
    "\n",
    "# Naive Bayes Bernoulli \n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, y_train)\n",
    "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Naive Bayes Bernoulli:\")\n",
    "print(\"\\nAccuracy: %1.3f\" % clf.score(X_test, y_test))\n",
    "print(\"Precision: %1.3f\" % metrics.precision_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"Recall: %1.3f\" % metrics.recall_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"F1-Score: %1.3f\" % metrics.f1_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"-----------------\\n\")\n",
    "\n",
    "# Some warnings are presented because we got some negative values with NBB.\n",
    "# TODO: Fix it? \n",
    "\n",
    "# Decision Trees\n",
    "clf = DecisionTreeClassifier(random_state = 42) # Feel free to change 'min_samples_split' \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Decision Trees:\")\n",
    "print(\"\\nAccuracy: %1.3f\" % clf.score(X_test, y_test))\n",
    "print(\"Precision: %1.3f\" % metrics.precision_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"Recall: %1.3f\" % metrics.recall_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"F1-Score: %1.3f\" % metrics.f1_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"-----------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
