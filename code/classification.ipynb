{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57221048 0.71836518 1.71079118 0.55571547 1.65751708 0.9930963 ]\n",
      " [0.73236606 0.07461438 1.05744451 1.48034787 1.00569838 0.88175599]\n",
      " [0.06248437 0.37559931 1.23910033 0.89870615 0.22293022 0.83663876]\n",
      " ...\n",
      " [0.73092353 1.57890977 1.05808091 1.25729583 1.00774584 0.75079358]\n",
      " [0.73112794 1.58161404 1.05974868 1.68251831 1.00463818 0.00652814]\n",
      " [0.73121985 1.59107898 1.06231303 1.24268337 1.00577343 1.52790048]]\n",
      "(array([    3,  1801,  1802, ..., 21972, 21974, 21976]), array([5, 5, 5, ..., 3, 5, 3]))\n",
      "Logistic Regression:\n",
      "\n",
      "Accuracy: 0.931\n",
      "Precision: 0.936\n",
      "Recall: 0.931\n",
      "F1-Score: 0.927\n",
      "-----------------\n",
      "\n",
      "Naive Bayes Bernoulli:\n",
      "\n",
      "Accuracy: 0.616\n",
      "Precision: 0.622\n",
      "Recall: 0.616\n",
      "F1-Score: 0.544\n",
      "-----------------\n",
      "\n",
      "Decision Trees:\n",
      "\n",
      "Accuracy: 0.980\n",
      "Precision: 0.980\n",
      "Recall: 0.980\n",
      "F1-Score: 0.980\n",
      "-----------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klajdi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/klajdi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "#--------------------------------#\n",
    "# To use later\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "# To use later\n",
    "#--------------------------------#\n",
    "    \n",
    "# Encode labels with value 0-> n_classes -1\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Read excel with Pandas library\n",
    "path_to_dataset = \"../datasets/one_appliance.xls\"\n",
    "df = pd.read_excel(path_to_dataset)\n",
    "\n",
    "# These columns will be our features - X\n",
    "features = ['I50', 'Φ50', 'I150', 'Φ150', 'I250', 'Φ250']\n",
    "\n",
    "# Features - X\n",
    "X = df[features]\n",
    "\n",
    "# Calculate Z-score in order to find outliers\n",
    "z = np.abs(stats.zscore(X))\n",
    "print(z) # Visualize\n",
    "threshold = 2.5 # Change the threshold arbitrarily\n",
    "print(np.where(z > threshold))\n",
    "df = df[(z < threshold).all(axis=1)] # Remove outliers that exceed the threshold given from dataset\n",
    "\n",
    "# Now get as X the 'clean' features\n",
    "X = df[features]\n",
    "\n",
    "# Scale the inputs (4 options; put comment in the ones you are not using)\n",
    "\n",
    "# -1- Standard Scaling\n",
    "#scaler = StandardScaler()\n",
    "#X = scaler.fit_transform(X)\n",
    "\n",
    "# -2- Min-max scaling \n",
    "#scaler = preprocessing.MinMaxScaler()\n",
    "#X = scaler.fit_transform(X)\n",
    "\n",
    "# -3- Robust scaling\n",
    "scaler = preprocessing.RobustScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# -4- Normalizing\n",
    "#scaler = preprocessing.Normalizer()\n",
    "#X = scaler.fit_transform(X)\n",
    "\n",
    "# Label - Y\n",
    "y = df['appliance']\n",
    "\n",
    "# Convert y to integer \n",
    "le.fit(y) # Fit label encoder\n",
    "y = le.transform(y) # Transform labels to normalized encoding\n",
    "\n",
    "# Split to train & test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform Logistic Regression\n",
    "reg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"\\nAccuracy: %1.3f\" % reg.score(X_test, y_test))\n",
    "print(\"Precision: %1.3f\" % metrics.precision_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"Recall: %1.3f\" % metrics.recall_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"F1-Score: %1.3f\" % metrics.f1_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"-----------------\\n\")\n",
    "\n",
    "# Bayesian Ridge\n",
    "#bayess = BayesianRidge().fit(X_train, y_train)\n",
    "\n",
    "#y_pred = bayess.predict(X_test)\n",
    "#print(\"Bayesian Ridge:\\n\")\n",
    "#print(\"Accuracy: %1.3f\" % bayess.score(X_test, y_test))\n",
    "#print(\"Precision: %1.3f\" % metrics.precision_score(y_test, y_pred, average ='weighted'))\n",
    "#print(\"Recall: %1.3f\" % metrics.recall_score(y_test, y_pred, average ='weighted'))\n",
    "#print(\"F1-Score: %1.3f\" % metrics.f1_score(y_test, y_pred, average ='weighted'))\n",
    "#print(\"-----------------\\n\")\n",
    "\n",
    "# Naive Bayes Bernoulli \n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, y_train)\n",
    "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Naive Bayes Bernoulli:\")\n",
    "print(\"\\nAccuracy: %1.3f\" % clf.score(X_test, y_test))\n",
    "print(\"Precision: %1.3f\" % metrics.precision_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"Recall: %1.3f\" % metrics.recall_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"F1-Score: %1.3f\" % metrics.f1_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"-----------------\\n\")\n",
    "\n",
    "# Some warnings are presented because we got some negative values with NBB.\n",
    "# TODO: Fix it? \n",
    "\n",
    "# Decision Trees\n",
    "clf = DecisionTreeClassifier(random_state = 42) # Feel free to change 'min_samples_split' \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Decision Trees:\")\n",
    "print(\"\\nAccuracy: %1.3f\" % clf.score(X_test, y_test))\n",
    "print(\"Precision: %1.3f\" % metrics.precision_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"Recall: %1.3f\" % metrics.recall_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"F1-Score: %1.3f\" % metrics.f1_score(y_test, y_pred, average ='weighted'))\n",
    "print(\"-----------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
